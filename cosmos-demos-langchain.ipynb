{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we'll demonstrate how to leverage a sample dataset stored in Azure Cosmos DB for MongoDB vCore to ground OpenAI models. We'll do this taking advantage of Azure Cosmos DB for Mongo DB vCore's [vector similarity search](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search) functionality. In the end, we'll create an interatice chat session with the GPT-3.5 completions model to answer questions about Azure services informed by our dataset. This process is known as Retrieval Augmented Generation, or RAG.\n",
    "\n",
    "This tutorial borrows some code snippets and example data from the Azure Cognitive Search Vector Search demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries <a class=\"anchor\" id=\"preliminaries\"></a>\n",
    "First, let's start by installing the packages that we'll need later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install openai\n",
    "# ! pip install pymongo\n",
    "# ! pip install python-dotenv\n",
    "# ! pip install azure-storage-blob\n",
    "# ! pip install json\n",
    "# ! pip install ijson\n",
    "# from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "from dotenv import dotenv_values\n",
    "import pymongo\n",
    "#from azure.storage.blob import BlobServiceClient\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import AzureOpenAI as AOAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.chat_message_histories import MongoDBChatMessageHistory\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "from langchain_community.document_loaders import JSONLoader # Check if really need\n",
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    AzureCosmosDBVectorSearch,\n",
    "    CosmosDBSimilarityType,\n",
    "    CosmosDBVectorSearchType,\n",
    ")\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the example.env as a template to provide the necessary keys and endpoints in your own .env file.\n",
    "Make sure to modify the env_name accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"fabcondemo.env\" # following example.env template change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "mongo_conn = config['mongo_connection_string']\n",
    "mongo_database = config['mongo_database_name']\n",
    "mongo_collection = config['mongo_collection_name']\n",
    "mongo_cache_collection = config['mongo_cache_collection_name']\n",
    "mongo_client = pymongo.MongoClient(mongo_conn)\n",
    "\n",
    "openai_endpoint = config['openai_endpoint']\n",
    "openai_key = config['openai_key']\n",
    "openai_version = config['openai_version']\n",
    "openai_embeddings_deployment = config['openai_embeddings_deployment']\n",
    "openai_embeddings_model = config['openai_embeddings_model']\n",
    "openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])\n",
    "openai_completions_deployment = config['openai_completions_deployment']\n",
    "openai_completions_model = config['openai_completions_model']\n",
    "\n",
    "from langchain_community.cache import AzureCosmosDBSemanticCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import urllib \n",
    "\n",
    "dimensions = 1536\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_version)\n",
    "\n",
    "azure_openai_embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=openai_embeddings_deployment, api_key=openai_key, azure_endpoint=openai_endpoint,  model=openai_embeddings_model,dimensions=dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Set up the MongoDB vCore database and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database FabConfDB\n",
    "db = mongo_client[mongo_database]\n",
    "\n",
    "# Get the collection FabConfCollection\n",
    "collection = db[mongo_collection]\n",
    "# Get the collection CacheCollection\n",
    "# cache = db[mongo_cache_collection]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to generate embeddings\n",
    "\n",
    "This is used to vectorize the user input for the vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(10))\n",
    "def generate_embeddings(text):\n",
    "    '''\n",
    "    Generate embeddings from string of text.\n",
    "    This will be used to vectorize data and user input for interactions with Azure OpenAI.\n",
    "    '''\n",
    "    # OpenAI asks for a model but it's actually a deployment.\n",
    "    response = openai_client.embeddings.create(input = text, model = openai_embeddings_deployment, dimensions= dimensions)\n",
    "    #response = azure_openai_embeddings.embed_query(text) #LangChain example from Jasmine\n",
    "    embeddings = response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding generation function\n",
    "emb = generate_embeddings('test')\n",
    "emb[0:5], len(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing vector search w/ LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdb = AzureCosmosDBVectorSearch(text_key=\"content\", embedding_key=\"contentVector\", collection=mongo_collection, embedding=azure_openai_embeddings)\n",
    "vectorstore = cdb.from_connection_string(\n",
    "connection_string=mongo_conn, namespace=mongo_database+\".\"+mongo_collection, embedding=azure_openai_embeddings)\n",
    "\n",
    "# Redfine where embedding and text data is stored\n",
    "vectorstore._embedding_key = \"contentVector\"\n",
    "vectorstore._text_key = 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to reset the index\n",
    "#collection.drop_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lists = 100\n",
    "similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "kind = CosmosDBVectorSearchType.VECTOR_HNSW\n",
    "m = 16\n",
    "ef_construction = 64\n",
    "ef_search = 40\n",
    "score_threshold = 0.1\n",
    "\n",
    "vectorstore.create_index(\n",
    "    num_lists, dimensions, similarity_algorithm, kind, m, ef_construction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vector search and document retrieval\n",
    "vectorstore.similarity_search(\"Azure SQL databases\", k=10, score_threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding RAG to chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Always list out at least 3 options if the user asks for information about Azure. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(   \n",
    "    llm= AzureChatOpenAI(azure_endpoint=openai_endpoint,\n",
    "                        api_key=openai_key,\n",
    "                        api_version=openai_version,\n",
    "                        azure_deployment=openai_completions_deployment),\n",
    "    # llm = llm,\n",
    "chain_type=\"stuff\",\n",
    "    retriever=qa_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = qa({\"query\": \"Tell me about NoSQL or nonrelational databases in Azure\"})\n",
    "print(docs[\"result\"])\n",
    "print(docs[\"source_documents\"])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding semantic caching to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10},\n",
    ")\n",
    "\n",
    "sem_qa = RetrievalQA.from_chain_type(\n",
    "    llm= AOAI(azure_endpoint=openai_endpoint,\n",
    "                        api_key=openai_key,\n",
    "                        api_version=openai_version,\n",
    "                        azure_deployment=\"completions\",#openai_completions_deployment\",\n",
    "                        cache=True,n=2,best_of=2),\n",
    "    # llm = llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=qa_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "\n",
    "# sem_qa = RetrievalQA.from_chain_type(\n",
    "#     llm= AzureChatOpenAI(azure_endpoint=openai_endpoint,\n",
    "#                         api_key=openai_key,\n",
    "#                         api_version=openai_version,\n",
    "#                         azure_deployment=openai_completions_deployment,\n",
    "#                         cache=True,n=3),\n",
    "#     # llm = llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=qa_retriever,\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs={\"prompt\": PROMPT},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lists = 1\n",
    "dimensions = 1536\n",
    "similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "kind = CosmosDBVectorSearchType.VECTOR_IVF\n",
    "m = 16\n",
    "ef_construction = 64\n",
    "ef_search = 40\n",
    "score_threshold = 0.98\n",
    "\n",
    "sem_cache = AzureCosmosDBSemanticCache(\n",
    "        cosmosdb_connection_string=mongo_conn,\n",
    "        cosmosdb_client=None,\n",
    "        embedding=azure_openai_embeddings,\n",
    "        database_name=\"ExampleDB\",\n",
    "        collection_name=\"chatcache\",\n",
    "        num_lists=num_lists,\n",
    "        similarity=similarity_algorithm,\n",
    "        kind=kind,\n",
    "        dimensions=dimensions,\n",
    "        m=m,\n",
    "        ef_construction=ef_construction,\n",
    "        ef_search=ef_search,\n",
    "        score_threshold=score_threshold)\n",
    "\n",
    "set_llm_cache(\n",
    "    sem_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db['cache_collection'].index_information()\n",
    "db['chatcache'].drop_indexes()\n",
    "db.drop_collection('chatcache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_qa('Tell me about NoSQL or nonrelational databases in Azure') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_qa('Tell me about NoSQL or nonrelational databases in Azure') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything after this line is experimental and a work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message_history = MongoDBChatMessageHistory(\n",
    "    session_id=\"test_session\",\n",
    "    connection_string=mongo_conn,\n",
    "    database_name=mongo_database,\n",
    "    collection_name=\"chat_histories\",\n",
    ")\n",
    "\n",
    "chat_message_history.add_user_message(\"Hello\")\n",
    "chat_message_history.add_ai_message(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Always list out at least 3 options if the user asks for information about Azure. \n",
    "\n",
    "{context},\n",
    "\n",
    "This is the history of previous conversations. Use this to help answer the question.\n",
    "\n",
    "{history},\n",
    "\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate.from_template(template=prompt_template) #, input_variables=[\"context\", \"history\", \"question\"])\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# p = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are a helpful assistant.\"),\n",
    "#         MessagesPlaceholder(variable_name=\"history\"),\n",
    "#         (\"human\", \"{question}\"),\n",
    "#     ] ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chains\n",
    "\n",
    "qa_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "\n",
    "hist_llm = AzureChatOpenAI(azure_endpoint=openai_endpoint,\n",
    "                        api_key=openai_key,\n",
    "                        api_version=openai_version,\n",
    "                        azure_deployment=openai_completions_deployment)\n",
    "\n",
    "mongo_history = MongoDBChatMessageHistory(\n",
    "    session_id=\"test_session\",\n",
    "    connection_string=mongo_conn,\n",
    "    database_name=mongo_database,\n",
    "    collection_name=\"chat_histories\")\n",
    "\n",
    "chat_message_history = ConversationBufferMemory(\n",
    "    chat_memory = mongo_history,\n",
    "    input_key='question',\n",
    "    memory_key='history',\n",
    "    llm=hist_llm,\n",
    "    output_key='result',\n",
    "    return_messages=True)\n",
    "\n",
    "chathist_qa = RetrievalQA.from_chain_type(hist_llm,\n",
    "    #chain_type=\"stuff\",\n",
    "    retriever=qa_retriever,\n",
    "    return_source_documents=True,\n",
    "    memory= chat_message_history,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT, \"verbose\" :True, \"memory\": chat_message_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lists = 1\n",
    "dimensions = 1536\n",
    "similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "kind = CosmosDBVectorSearchType.VECTOR_IVF\n",
    "m = 16\n",
    "ef_construction = 64\n",
    "ef_search = 40\n",
    "score_threshold = .99\n",
    "\n",
    "sem_cache = AzureCosmosDBSemanticCache(\n",
    "        cosmosdb_connection_string=mongo_conn,\n",
    "        cosmosdb_client=None,\n",
    "        embedding=azure_openai_embeddings,\n",
    "        database_name=\"ExampleDB\",\n",
    "        collection_name=\"chatcache\",\n",
    "        num_lists=num_lists,\n",
    "        similarity=similarity_algorithm,\n",
    "        kind=kind,\n",
    "        dimensions=dimensions,\n",
    "        m=m,\n",
    "        ef_construction=ef_construction,\n",
    "        ef_search=ef_search,\n",
    "        score_threshold=score_threshold)\n",
    "\n",
    "set_llm_cache(\n",
    "    sem_cache\n",
    ")\n",
    "set_llm_cache(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for chain with only history/memory\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "hist_llm = AzureChatOpenAI(azure_endpoint=openai_endpoint,\n",
    "                        api_key=openai_key,\n",
    "                        api_version=openai_version,\n",
    "                        azure_deployment=openai_completions_deployment)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "hist_llm = AzureChatOpenAI(azure_endpoint=openai_endpoint,\n",
    "                        api_key=openai_key,\n",
    "                        api_version=openai_version,\n",
    "                        azure_deployment=openai_completions_deployment, temperature=0)\n",
    "\n",
    "chain = prompt | hist_llm\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: MongoDBChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        connection_string=mongo_conn,\n",
    "        database_name=mongo_database,\n",
    "        collection_name=\"chat_histories\",\n",
    "    ),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"<SESSION_ID>\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing memory. This chain has only Memory, no RAG and no Semantic caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history.invoke({\"question\": \"My name is James.\"}, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history.invoke({\"question\": \"What did I tell you was my name? Explain it\"}, config=config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
