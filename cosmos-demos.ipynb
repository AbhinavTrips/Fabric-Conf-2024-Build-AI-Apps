{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this demo, we'll demonstrate how to leverage a sample dataset stored in Azure Cosmos DB for MongoDB to ground OpenAI models. We'll do this taking advantage of Azure Cosmos DB for Mongo DB's [vector similarity search](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search) functionality. In the end, we'll create an interatice chat session with the GPT-3.5 completions model to answer questions about Azure services informed by our dataset. This process is known as Retrieval Augmented Generation, or RAG.\n",
    "\n",
    "This tutorial borrows some code snippets and example data from the Azure Cognitive Search Vector Search demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries <a class=\"anchor\" id=\"preliminaries\"></a>\n",
    "First, let's start by installing the packages that we'll need later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai\n",
    "! pip install pymongo\n",
    "! pip install python-dotenv\n",
    "! pip install json\n",
    "! pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "from dotenv import dotenv_values\n",
    "import pymongo\n",
    "from openai import AzureOpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the example.env as a template to provide the necessary keys and endpoints in your own .env file.\n",
    "Make sure to modify the env_name accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"fabcondemo.env\" # following example.env template change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "\n",
    "mongo_conn = config['mongo_connection_string']\n",
    "mongo_database = config['mongo_database_name']\n",
    "mongo_collection = config['mongo_collection_name']\n",
    "mongo_vector_property = config['mongo_vector_property_name']\n",
    "mongo_cache = config['mongo_cache_collection_name']\n",
    "mongo_semcache = config['mongo_semcache_collection_name']\n",
    "mongo_chat_history = config['mongo_chathistory_collection_name']\n",
    "# Create the MongoDB client\n",
    "mongo_client = pymongo.MongoClient(mongo_conn)\n",
    "\n",
    "\n",
    "openai_endpoint = config['openai_endpoint']\n",
    "openai_key = config['openai_key']\n",
    "openai_version = config['openai_version']\n",
    "openai_embeddings_deployment = config['openai_embeddings_deployment']\n",
    "openai_embeddings_model = config['openai_embeddings_model']\n",
    "openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])\n",
    "openai_completions_deployment = config['openai_completions_deployment']\n",
    "openai_completions_model = config['openai_completions_model']\n",
    "# Create the OpenAI client\n",
    "openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB database and collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database\n",
    "database = mongo_client[mongo_database]\n",
    "\n",
    "# Get the movie collection\n",
    "movies = database[mongo_collection]\n",
    "\n",
    "# Get the cache collection\n",
    "cache = database[mongo_cache]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embeddings from Azure OpenAI\n",
    "\n",
    "This is used to vectorize the user input for the vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text):\n",
    "    '''\n",
    "    Generate embeddings from string of text.\n",
    "    This will be used to vectorize data and user input for interactions with Azure OpenAI.\n",
    "    '''\n",
    "    # OpenAI asks for a model but it's actually a deployment.\n",
    "    response = openai_client.embeddings.create(input = text, model = openai_embeddings_deployment, dimensions = openai_embeddings_dimensions)\n",
    "\n",
    "    embeddings = response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search in Azure Cosmos DB for MongoDB\n",
    "\n",
    "This defines a function for performing a vector search over any collection in Azure Cosmos DB for MongoDB with a similarly configured vector index. Takes a collection reference, array of vectors, and optional similarity score to filter for top matches and number of results to return to filter further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to assist with vector search\n",
    "def vector_search(collection, query_embedding, similarity_score=0.02, num_results=3):\n",
    "    \n",
    "    pipeline = [\n",
    "        {\n",
    "            '$search': {\n",
    "                \"cosmosSearch\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"path\": mongo_vector_property,\n",
    "                    \"k\": num_results,\n",
    "                    \"efsearch\": 40 # optional for HNSW only \n",
    "                },\n",
    "                \"returnStoredSource\": True }},\n",
    "        {'$project': { 'similarityScore': { '$meta': 'searchScore' }, 'document' : '$$ROOT' } },\n",
    "        { '$match': { \"similarityScore\": { '$gt': similarity_score } } }\n",
    "    ]\n",
    "\n",
    "    results = list(collection.aggregate(pipeline))\n",
    "\n",
    "    # Exclude the 'vector' and _id properties from the results. Makes Generation easier\n",
    "    #for result in results:\n",
    "    #    result.pop(\"document.vector\", None)\n",
    "    #    result.pop(\"document._id\", None)\n",
    "    \n",
    "    for result in results:\n",
    "        del result['document']['vector']\n",
    "        del result['_id']\n",
    "        del result['document']['_id']\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a test query below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What movie has Buzz Lightyear?\"\n",
    "embeddings = generate_embeddings(query)\n",
    "results = vector_search(movies, embeddings)\n",
    "for result in results: \n",
    "    print(result)\n",
    "    print(f\"Similarity Score: {result['similarityScore']}\")  \n",
    "    print(f\"Title: {result['document']['title']}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function to get recent chat history\n",
    "\n",
    "This provides conversational context to the LLM, allowing it to better have a conversation with the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab chat history to as part of the payload to GPT model for completion.\n",
    "def get_chat_history(completions=3):\n",
    "\n",
    "    # Sort by _id in descending order and limit the results to 3\n",
    "    results = cache.find({}, {\"prompt\": 1, \"completion\": 1}).sort([(\"_id\", -1)]).limit(completions)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function helps to ground the model with prompts and system instructions.\n",
    "\n",
    "def generate_completion(user_prompt, vector_search_results, chat_history):\n",
    "    \n",
    "    system_prompt = '''\n",
    "    You are an intelligent assistant for the Movie Lens Expert AI Assistant.\n",
    "    You are designed to provide helpful answers to user questions about movies in your database.\n",
    "    You are friendly, helpful, and informative and wee bit cheeky.\n",
    "        - Only answer questions related to the information provided below.\n",
    "        - Write two lines of whitespace between each answer in the list.\n",
    "        - If you're unsure of an answer, you can say \"\"I don't know\"\" or \"\"I'm not sure\"\" and recommend users search themselves.\"\n",
    "    '''\n",
    "\n",
    "    # Create a list of messages as a payload to send to the OpenAI Completions API\n",
    "\n",
    "    # System Prompt\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    # Chat history\n",
    "    for item in chat_history:\n",
    "        messages.append({\"role\": \"system\", \"content\": item['prompt'] + \" \" + item['completion']})\n",
    "\n",
    "    # User Prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # Add the vector search results\n",
    "    for item in vector_search_results:\n",
    "        messages.append({\"role\": \"system\", \"content\": json.dumps(item['document'])})\n",
    "\n",
    "    # Send the payload to the OpenAI Completions API\n",
    "    response = openai_client.chat.completions.create(model = openai_completions_deployment, messages = messages)\n",
    "   \n",
    "    \n",
    "    return response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache the Chat History\n",
    "\n",
    "Save the user prompts and completions in a conversation. \n",
    "\n",
    "This can be used to answer the same questions from other users. Generally this is more efficient and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chat_history(user_prompt, prompt_vectors, response):\n",
    "\n",
    "    chat = [\n",
    "        {\n",
    "            'prompt': user_prompt,\n",
    "            'completion': response['choices'][0]['message']['content'],\n",
    "            'completionTokens': str(response['usage']['completion_tokens']),\n",
    "            'promptTokens': str(response['usage']['prompt_tokens']),\n",
    "            'totalTokens': str(response['usage']['total_tokens']),\n",
    "            'model': response['model'],\n",
    "            mongo_vector_property: prompt_vectors\n",
    "         }\n",
    "    ]\n",
    "\n",
    "    cache.insert_one(chat[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put It All Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop of user input and model output. You can now perform Q&A over the sample data!\n",
    "\n",
    "user_input = \"\"\n",
    "\n",
    "print(\"*** Please ask your model questions about Movies. Type 'end' to end the session.\\n\")\n",
    "\n",
    "user_input = input(\"Prompt: \")\n",
    "\n",
    "while user_input.lower() != \"end\":\n",
    "    \n",
    "    # Generate embeddings from the user input\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "\n",
    "    # Query the chat history cache first to see if this question has been asked before\n",
    "    cache_results = vector_search(cache, user_embeddings, similarity_score=0.99, num_results=1)\n",
    "\n",
    "    if len(cache_results) > 0:\n",
    "\n",
    "        #print the user input and cached result. Confirm this is what they were asking for.\n",
    "        print(\"\\n\" + user_input)\n",
    "        print(cache_results[0]['document']['completion'])\n",
    "        user_input_cache_hit_ok = input(\"Is this what you were asking for? (yes/no): \")\n",
    "        \n",
    "        if user_input_cache_hit_ok.lower() == \"no\":   \n",
    "        \n",
    "            # Perform a vector search on the user input\n",
    "            search_results = vector_search(movies, user_embeddings)\n",
    "\n",
    "            # Get recent chat history to send to GPT model\n",
    "            chat_history = get_chat_history(3)\n",
    "            \n",
    "            # Generate completion based on the search results and user input with chat history for context\n",
    "            completions_results = generate_completion(user_input, search_results, chat_history)\n",
    "\n",
    "            # print the user input\n",
    "            print(\"\\n\" + user_input)\n",
    "\n",
    "            # Print the generated LLM completions\n",
    "            print(completions_results['choices'][0]['message']['content'])\n",
    "\n",
    "            # Cache the conversation\n",
    "            save_chat_history(user_input, user_embeddings, completions_results)\n",
    "        \n",
    "\n",
    "    # Ask for more user input\n",
    "    user_input = input(\"Prompt: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function for the LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(user_input):\n",
    "\n",
    "    # Define the response payload\n",
    "    response_payload = {\"completion\": \"None\", \"elapsed_time\": 0, \"cache_hit\": False}\n",
    "\n",
    "    # Generate embeddings from the user input\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "\n",
    "    # Create a timer to measure the time it takes to complete the request\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Query the chat history cache first to see if this question has been asked before\n",
    "    cache_results = vector_search(cache, user_embeddings, similarity_score=0.99, num_results=1)\n",
    "\n",
    "    if len(cache_results) > 0:\n",
    "        \n",
    "        # Stop the timer\n",
    "        end_time = time.time()\n",
    "        \n",
    "        response_payload['completion'] = cache_results[0]['document']['completion']\n",
    "        response_payload['cache_hit'] = True\n",
    "        \n",
    "    else:\n",
    "        # Restart the timer to measure the time it takes to complete doing vector search and generate a completion\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Perform a vector search on the user input\n",
    "        search_results = vector_search(movies, user_embeddings)\n",
    "\n",
    "        # Get recent chat history to send to GPT model\n",
    "        chat_history = get_chat_history(3)\n",
    "        \n",
    "        # Generate completion based on the search results and user input with chat history for context\n",
    "        completions_results = generate_completion(user_input, search_results, chat_history)\n",
    "\n",
    "        # Stop the timer\n",
    "        end_time = time.time()\n",
    "\n",
    "        response_payload['completion'] = completions_results['choices'][0]['message']['content']\n",
    "\n",
    "        # Cache the conversation\n",
    "        save_chat_history(user_input, user_embeddings, completions_results)\n",
    "\n",
    "    \n",
    "    # Format the elapsed time to milliseconds\n",
    "    response_payload['elapsed_time'] = round((end_time - start_time) * 1000, 2)\n",
    "\n",
    "    # Return the generated LLM completion or cache results\n",
    "    return response_payload\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make a UX for this thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Ask me anything about movies!\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, chat_history):\n",
    "        \n",
    "        # Get LLM completion\n",
    "        response_payload = chat_completion(user_message)\n",
    "\n",
    "        response = response_payload['completion']\n",
    "        cache_hit = response_payload['cache_hit']\n",
    "        elapsed_time = response_payload['elapsed_time']\n",
    "        \n",
    "        # Append user message and response to chat history\n",
    "        chat_history.append([user_message, response + f\"\\n (Time: {elapsed_time}ms, Cache Hit: {cache_hit})\"])\n",
    "        \n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    \n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
