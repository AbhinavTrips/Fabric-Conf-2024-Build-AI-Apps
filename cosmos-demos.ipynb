{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we'll demonstrate how to leverage a sample dataset stored in Azure Cosmos DB for MongoDB vCore to ground OpenAI models. We'll do this taking advantage of Azure Cosmos DB for Mongo DB vCore's [vector similarity search](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search) functionality. In the end, we'll create an interatice chat session with the GPT-3.5 completions model to answer questions about Azure services informed by our dataset. This process is known as Retrieval Augmented Generation, or RAG.\n",
    "\n",
    "This tutorial borrows some code snippets and example data from the Azure Cognitive Search Vector Search demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries <a class=\"anchor\" id=\"preliminaries\"></a>\n",
    "First, let's start by installing the packages that we'll need later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai\n",
    "! pip install pymongo\n",
    "! pip install python-dotenv\n",
    "! pip install azure-storage-blob\n",
    "! pip install json\n",
    "! pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "from dotenv import dotenv_values\n",
    "import pymongo\n",
    "#from azure.storage.blob import BlobServiceClient\n",
    "from openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the example.env as a template to provide the necessary keys and endpoints in your own .env file.\n",
    "Make sure to modify the env_name accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"fabcondemo.env\" # following example.env template change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "mongo_conn = config['mongo_connection_string']\n",
    "mongo_database = config['mongo_database_name']\n",
    "mongo_collection = config['mongo_collection_name']\n",
    "mongo_cache_collection = config['mongo_cache_collection_name']\n",
    "mongo_client = pymongo.MongoClient(mongo_conn)\n",
    "\n",
    "openai_endpoint = config['openai_endpoint']\n",
    "openai_key = config['openai_key']\n",
    "openai_version = config['openai_version']\n",
    "openai_embeddings_deployment = config['openai_embeddings_deployment']\n",
    "openai_embeddings_model = config['openai_embeddings_model']\n",
    "openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])\n",
    "openai_completions_deployment = config['openai_completions_deployment']\n",
    "openai_completions_model = config['openai_completions_model']\n",
    "openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Set up the MongoDB vCore database and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database FabConfDB\n",
    "db = mongo_client[mongo_database]\n",
    "\n",
    "# Get the collection FabConfCollection\n",
    "collection = db[mongo_collection]\n",
    "# Get the collection CacheCollection\n",
    "cache = db[mongo_cache_collection]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to generate embeddings\n",
    "\n",
    "This is used to vectorize the user input for the vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(10))\n",
    "def generate_embeddings(text):\n",
    "    '''\n",
    "    Generate embeddings from string of text.\n",
    "    This will be used to vectorize data and user input for interactions with Azure OpenAI.\n",
    "    '''\n",
    "    # OpenAI asks for a model but it's actually a deployment.\n",
    "    response = openai_client.embeddings.create(input = text, model = openai_embeddings_deployment, dimensions= openai_embeddings_dimensions)\n",
    "\n",
    "    embeddings = response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search in Cosmos DB for MongoDB vCore\n",
    "\n",
    "This defines a function for performing a vector search over data in Azure Cosmos DB for MongoDB vCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to assist with vector search\n",
    "def vector_search(query, num_results=3):\n",
    "\n",
    "    query_embedding = query # generate_embeddings(query)\n",
    "        \n",
    "    pipeline = [\n",
    "        {\n",
    "            '$search': {\n",
    "                \"cosmosSearch\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"path\": \"contentVector\",\n",
    "                    \"k\": num_results #, \"efsearch\": 40 # optional for HNSW only \n",
    "                },\n",
    "                \"returnStoredSource\": True }},\n",
    "        {'$project': { 'similarityScore': { '$meta': 'searchScore' }, 'document' : '$$ROOT' } }\n",
    "    ]\n",
    "\n",
    "    results = collection.aggregate(pipeline)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a test query below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.5570008226634062\n",
      "Title: Azure Machine Learning\n",
      "Content: Azure Machine Learning is a cloud-based service for building, training, and deploying machine learning models. It offers a visual interface for creating and managing experiments, as well as support for popular programming languages like Python and R. Machine Learning supports various algorithms, frameworks, and data sources, making it easy to integrate with your existing data and workflows. You can deploy your models as web services, and scale them based on your needs. It also integrates with other Azure services, such as Azure Databricks and Azure Data Factory.\n",
      "Category: AI + Machine Learning\n",
      "\n",
      "Similarity Score: 0.529827014500708\n",
      "Title: Azure Machine Learning\n",
      "Content: Azure Machine Learning is a fully managed, end-to-end platform that enables you to build, train, and deploy machine learning models at scale. It provides features like automated machine learning, data labeling, and model management. Machine Learning supports various frameworks, such as TensorFlow, PyTorch, and Scikit-learn. You can use Azure Machine Learning to develop predictive analytics solutions, improve your decision-making, and gain insights into your data. It also integrates with other Azure services, such as Azure Databricks and Azure Synapse Analytics.\n",
      "Category: AI + Machine Learning\n",
      "\n",
      "Similarity Score: 0.48332695513906687\n",
      "Title: Azure Cognitive Services\n",
      "Content: Azure Cognitive Services is a collection of AI services and APIs that enable you to build intelligent applications using pre-built models and algorithms. It provides features like computer vision, speech recognition, and natural language processing. Cognitive Services supports various platforms, such as .NET, Java, Node.js, and Python. You can use Azure Cognitive Services to build chatbots, analyze images and videos, and process and understand text. It also integrates with other Azure services, such as Azure Machine Learning and Azure Cognitive Search.\n",
      "Category: AI + Machine Learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the services for running ML models?\"\n",
    "embeddings = generate_embeddings(query)\n",
    "results = vector_search(embeddings)\n",
    "for result in results: \n",
    "    #print(result)\n",
    "    print(f\"Similarity Score: {result['similarityScore']}\")  \n",
    "    print(f\"Title: {result['document']['title']}\")  \n",
    "    print(f\"Content: {result['document']['content']}\")  \n",
    "    print(f\"Category: {result['document']['category']}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A over the data with GPT\n",
    "\n",
    "Finally, we'll create a helper function to feed prompts into the `Completions` model. Then we'll create interactive loop where you can pose questions to the model and receive information grounded in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_history(completions=3):\n",
    "\n",
    "    # Sort by _id in descending order and limit the results to 3\n",
    "    results = cache.find({}, {\"prompt\": 1, \"completion\": 1}).sort([(\"_id\", -1)]).limit(completions)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function helps to ground the model with prompts and system instructions.\n",
    "\n",
    "def generate_completion(vector_search_results, user_prompt):\n",
    "    \n",
    "    system_prompt = '''\n",
    "    You are an intelligent assistant for Microsoft Azure services.\n",
    "    You are designed to provide helpful answers to user questions about Azure services given the information about to be provided.\n",
    "        - Only answer questions related to the information provided below, provide 3 clear suggestions in a list format.\n",
    "        - Write two lines of whitespace between each answer in the list.\n",
    "        - Only provide answers that have products that are part of Microsoft Azure and based on the content items.\n",
    "        - If you're unsure of an answer, you can say \"\"I don't know\"\" or \"\"I'm not sure\"\" and recommend users search themselves.\"\n",
    "    '''\n",
    "    # Create a list of messages as a payload to send to the OpenAI API\n",
    "\n",
    "    # System Prompt\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    # Add the conversation history\n",
    "    conversation_history = get_conversation_history(3)\n",
    "    for item in conversation_history:\n",
    "        messages.append({\"role\": \"system\", \"content\": item['prompt'] + \" \" + item['completion']})\n",
    "\n",
    "    # User Prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # Add the vector search results\n",
    "    for item in vector_search_results:\n",
    "        messages.append({\"role\": \"system\", \"content\": item['document']['content']})\n",
    "\n",
    "    response = openai_client.chat.completions.create(model = openai_completions_deployment, messages = messages, user= \"Mark\", )\n",
    "    \n",
    "    return response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_generation(user_prompt, user_embeddings, response):\n",
    "\n",
    "    chat = [\n",
    "        {\n",
    "            'prompt': user_prompt,\n",
    "            'completion': response['choices'][0]['message']['content'],\n",
    "            'completionTokens': str(response['usage']['completion_tokens']),\n",
    "            'promptTokens': str(response['usage']['prompt_tokens']),\n",
    "            'totalTokens': str(response['usage']['total_tokens']),\n",
    "            'model': response['model'],\n",
    "            'vectorContent': user_embeddings\n",
    "         }\n",
    "    ]\n",
    "\n",
    "    cache.insert_one(chat[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Please ask your model questions about Azure services. Type 'end' to end the session.\n",
      "\n",
      "\n",
      "what is a good nosql database?\n",
      "I don't know. It would be best to search for a good NoSQL database on the Microsoft Azure website as they offer a variety of database services to meet different needs and requirements.\n"
     ]
    }
   ],
   "source": [
    "# Create a loop of user input and model output. You can now perform Q&A over the sample data!\n",
    "\n",
    "user_input = \"\"\n",
    "\n",
    "print(\"*** Please ask your model questions about Azure services. Type 'end' to end the session.\\n\")\n",
    "\n",
    "user_input = input(\"Prompt: \")\n",
    "\n",
    "while user_input.lower() != \"end\":\n",
    "    \n",
    "    # Generate embeddings from the user input\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "    \n",
    "    # Perform a vector search on the user input\n",
    "    search_results = vector_search(user_embeddings)\n",
    "    \n",
    "    # Generate completions based on the search results and user input\n",
    "    completions_results = generate_completion(search_results, user_input)\n",
    "\n",
    "    # print the user input\n",
    "    print(\"\\n\" + user_input)\n",
    "\n",
    "    # Print the generated LLM completions\n",
    "    print(completions_results['choices'][0]['message']['content'])\n",
    "\n",
    "    # Cache the conversation\n",
    "    cache_generation(user_input, user_embeddings, completions_results)\n",
    "    \n",
    "    # Ask for more user input\n",
    "    user_input = input(\"Prompt: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
